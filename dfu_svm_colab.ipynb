{
  "cells": [
    {
      "cell_type": "code",
      "execution_count": 2,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "pyTDbQDDh_dF",
        "outputId": "28be230d-ad15-4cd5-d820-440ee4a432d3"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "📂 Dataset already extracted.\n"
          ]
        }
      ],
      "source": [
        "import zipfile\n",
        "import os\n",
        "\n",
        "zip_path = \"DFU_dataset.zip\"\n",
        "extract_to = \"DFU_dataset\"\n",
        "\n",
        "# Extract only if not already done\n",
        "if not os.path.exists(extract_to):\n",
        "    with zipfile.ZipFile(zip_path, 'r') as zip_ref:\n",
        "        zip_ref.extractall(extract_to)\n",
        "    print(\"✅ Dataset extracted to:\", extract_to)\n",
        "else:\n",
        "    print(\"📂 Dataset already extracted.\")\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 4,
      "metadata": {
        "id": "OXNOnZuZiFMu"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "✅ No rename needed or already renamed.\n"
          ]
        }
      ],
      "source": [
        "import os\n",
        "\n",
        "nested = \"DFU_dataset/DFU_dataset\"\n",
        "renamed = \"DFU_dataset/data\"\n",
        "\n",
        "if os.path.exists(nested):\n",
        "    os.rename(nested, renamed)\n",
        "    print(\"✅ Folder renamed to 'data'\")\n",
        "else:\n",
        "    print(\"✅ No rename needed or already renamed.\")\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 5,
      "metadata": {
        "id": "JXYEv9a5iHh4"
      },
      "outputs": [],
      "source": [
        "dataset_dir = \"DFU_dataset/data\""
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 7,
      "metadata": {},
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Collecting tensorflow\n",
            "  Using cached tensorflow-2.19.0-cp312-cp312-win_amd64.whl.metadata (4.1 kB)\n",
            "Collecting absl-py>=1.0.0 (from tensorflow)\n",
            "  Downloading absl_py-2.3.1-py3-none-any.whl.metadata (3.3 kB)\n",
            "Collecting astunparse>=1.6.0 (from tensorflow)\n",
            "  Using cached astunparse-1.6.3-py2.py3-none-any.whl.metadata (4.4 kB)\n",
            "Collecting flatbuffers>=24.3.25 (from tensorflow)\n",
            "  Using cached flatbuffers-25.2.10-py2.py3-none-any.whl.metadata (875 bytes)\n",
            "Collecting gast!=0.5.0,!=0.5.1,!=0.5.2,>=0.2.1 (from tensorflow)\n",
            "  Using cached gast-0.6.0-py3-none-any.whl.metadata (1.3 kB)\n",
            "Collecting google-pasta>=0.1.1 (from tensorflow)\n",
            "  Using cached google_pasta-0.2.0-py3-none-any.whl.metadata (814 bytes)\n",
            "Collecting libclang>=13.0.0 (from tensorflow)\n",
            "  Using cached libclang-18.1.1-py2.py3-none-win_amd64.whl.metadata (5.3 kB)\n",
            "Collecting opt-einsum>=2.3.2 (from tensorflow)\n",
            "  Using cached opt_einsum-3.4.0-py3-none-any.whl.metadata (6.3 kB)\n",
            "Requirement already satisfied: packaging in c:\\users\\veni\\anaconda\\lib\\site-packages (from tensorflow) (24.1)\n",
            "Requirement already satisfied: protobuf!=4.21.0,!=4.21.1,!=4.21.2,!=4.21.3,!=4.21.4,!=4.21.5,<6.0.0dev,>=3.20.3 in c:\\users\\veni\\anaconda\\lib\\site-packages (from tensorflow) (4.25.3)\n",
            "Requirement already satisfied: requests<3,>=2.21.0 in c:\\users\\veni\\anaconda\\lib\\site-packages (from tensorflow) (2.32.3)\n",
            "Requirement already satisfied: setuptools in c:\\users\\veni\\anaconda\\lib\\site-packages (from tensorflow) (75.1.0)\n",
            "Requirement already satisfied: six>=1.12.0 in c:\\users\\veni\\anaconda\\lib\\site-packages (from tensorflow) (1.16.0)\n",
            "Collecting termcolor>=1.1.0 (from tensorflow)\n",
            "  Using cached termcolor-3.1.0-py3-none-any.whl.metadata (6.4 kB)\n",
            "Requirement already satisfied: typing-extensions>=3.6.6 in c:\\users\\veni\\anaconda\\lib\\site-packages (from tensorflow) (4.11.0)\n",
            "Requirement already satisfied: wrapt>=1.11.0 in c:\\users\\veni\\anaconda\\lib\\site-packages (from tensorflow) (1.14.1)\n",
            "Collecting grpcio<2.0,>=1.24.3 (from tensorflow)\n",
            "  Using cached grpcio-1.73.1-cp312-cp312-win_amd64.whl.metadata (4.0 kB)\n",
            "Collecting tensorboard~=2.19.0 (from tensorflow)\n",
            "  Using cached tensorboard-2.19.0-py3-none-any.whl.metadata (1.8 kB)\n",
            "Collecting keras>=3.5.0 (from tensorflow)\n",
            "  Using cached keras-3.10.0-py3-none-any.whl.metadata (6.0 kB)\n",
            "Requirement already satisfied: numpy<2.2.0,>=1.26.0 in c:\\users\\veni\\anaconda\\lib\\site-packages (from tensorflow) (1.26.4)\n",
            "Requirement already satisfied: h5py>=3.11.0 in c:\\users\\veni\\anaconda\\lib\\site-packages (from tensorflow) (3.11.0)\n",
            "Collecting ml-dtypes<1.0.0,>=0.5.1 (from tensorflow)\n",
            "  Using cached ml_dtypes-0.5.1-cp312-cp312-win_amd64.whl.metadata (22 kB)\n",
            "Requirement already satisfied: wheel<1.0,>=0.23.0 in c:\\users\\veni\\anaconda\\lib\\site-packages (from astunparse>=1.6.0->tensorflow) (0.44.0)\n",
            "Requirement already satisfied: rich in c:\\users\\veni\\anaconda\\lib\\site-packages (from keras>=3.5.0->tensorflow) (13.7.1)\n",
            "Collecting namex (from keras>=3.5.0->tensorflow)\n",
            "  Using cached namex-0.1.0-py3-none-any.whl.metadata (322 bytes)\n",
            "Collecting optree (from keras>=3.5.0->tensorflow)\n",
            "  Using cached optree-0.16.0-cp312-cp312-win_amd64.whl.metadata (31 kB)\n",
            "Requirement already satisfied: charset-normalizer<4,>=2 in c:\\users\\veni\\anaconda\\lib\\site-packages (from requests<3,>=2.21.0->tensorflow) (3.3.2)\n",
            "Requirement already satisfied: idna<4,>=2.5 in c:\\users\\veni\\anaconda\\lib\\site-packages (from requests<3,>=2.21.0->tensorflow) (3.7)\n",
            "Requirement already satisfied: urllib3<3,>=1.21.1 in c:\\users\\veni\\anaconda\\lib\\site-packages (from requests<3,>=2.21.0->tensorflow) (2.2.3)\n",
            "Requirement already satisfied: certifi>=2017.4.17 in c:\\users\\veni\\anaconda\\lib\\site-packages (from requests<3,>=2.21.0->tensorflow) (2025.1.31)\n",
            "Requirement already satisfied: markdown>=2.6.8 in c:\\users\\veni\\anaconda\\lib\\site-packages (from tensorboard~=2.19.0->tensorflow) (3.4.1)\n",
            "Collecting tensorboard-data-server<0.8.0,>=0.7.0 (from tensorboard~=2.19.0->tensorflow)\n",
            "  Using cached tensorboard_data_server-0.7.2-py3-none-any.whl.metadata (1.1 kB)\n",
            "Requirement already satisfied: werkzeug>=1.0.1 in c:\\users\\veni\\anaconda\\lib\\site-packages (from tensorboard~=2.19.0->tensorflow) (3.0.3)\n",
            "Requirement already satisfied: MarkupSafe>=2.1.1 in c:\\users\\veni\\anaconda\\lib\\site-packages (from werkzeug>=1.0.1->tensorboard~=2.19.0->tensorflow) (2.1.3)\n",
            "Requirement already satisfied: markdown-it-py>=2.2.0 in c:\\users\\veni\\anaconda\\lib\\site-packages (from rich->keras>=3.5.0->tensorflow) (2.2.0)\n",
            "Requirement already satisfied: pygments<3.0.0,>=2.13.0 in c:\\users\\veni\\anaconda\\lib\\site-packages (from rich->keras>=3.5.0->tensorflow) (2.15.1)\n",
            "Requirement already satisfied: mdurl~=0.1 in c:\\users\\veni\\anaconda\\lib\\site-packages (from markdown-it-py>=2.2.0->rich->keras>=3.5.0->tensorflow) (0.1.0)\n",
            "Using cached tensorflow-2.19.0-cp312-cp312-win_amd64.whl (376.0 MB)\n",
            "Downloading absl_py-2.3.1-py3-none-any.whl (135 kB)\n",
            "Using cached astunparse-1.6.3-py2.py3-none-any.whl (12 kB)\n",
            "Using cached flatbuffers-25.2.10-py2.py3-none-any.whl (30 kB)\n",
            "Using cached gast-0.6.0-py3-none-any.whl (21 kB)\n",
            "Using cached google_pasta-0.2.0-py3-none-any.whl (57 kB)\n",
            "Using cached grpcio-1.73.1-cp312-cp312-win_amd64.whl (4.3 MB)\n",
            "Using cached keras-3.10.0-py3-none-any.whl (1.4 MB)\n",
            "Using cached libclang-18.1.1-py2.py3-none-win_amd64.whl (26.4 MB)\n",
            "Using cached ml_dtypes-0.5.1-cp312-cp312-win_amd64.whl (210 kB)\n",
            "Using cached opt_einsum-3.4.0-py3-none-any.whl (71 kB)\n",
            "Using cached tensorboard-2.19.0-py3-none-any.whl (5.5 MB)\n",
            "Using cached termcolor-3.1.0-py3-none-any.whl (7.7 kB)\n",
            "Using cached tensorboard_data_server-0.7.2-py3-none-any.whl (2.4 kB)\n",
            "Using cached namex-0.1.0-py3-none-any.whl (5.9 kB)\n",
            "Using cached optree-0.16.0-cp312-cp312-win_amd64.whl (315 kB)\n",
            "Installing collected packages: namex, libclang, flatbuffers, termcolor, tensorboard-data-server, optree, opt-einsum, ml-dtypes, grpcio, google-pasta, gast, astunparse, absl-py, tensorboard, keras, tensorflow\n",
            "Successfully installed absl-py-2.3.1 astunparse-1.6.3 flatbuffers-25.2.10 gast-0.6.0 google-pasta-0.2.0 grpcio-1.73.1 keras-3.10.0 libclang-18.1.1 ml-dtypes-0.5.1 namex-0.1.0 opt-einsum-3.4.0 optree-0.16.0 tensorboard-2.19.0 tensorboard-data-server-0.7.2 tensorflow-2.19.0 termcolor-3.1.0\n",
            "Note: you may need to restart the kernel to use updated packages.\n"
          ]
        }
      ],
      "source": [
        "pip install tensorflow\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 8,
      "metadata": {},
      "outputs": [],
      "source": [
        "import numpy as np\n",
        "import matplotlib.pyplot as plt\n",
        "import seaborn as sns\n",
        "from sklearn.preprocessing import LabelEncoder, StandardScaler\n",
        "from sklearn.model_selection import GridSearchCV\n",
        "from sklearn.svm import SVC\n",
        "from sklearn.metrics import classification_report, confusion_matrix, roc_curve, auc\n",
        "\n",
        "from tensorflow.keras.applications import MobileNetV2, ResNet50\n",
        "from tensorflow.keras.applications.mobilenet_v2 import preprocess_input as mobilenet_preprocess\n",
        "from tensorflow.keras.applications.resnet50 import preprocess_input as resnet_preprocess\n",
        "from tensorflow.keras.preprocessing.image import load_img, img_to_array\n",
        "\n",
        "img_size = 224\n",
        "mobilenet = MobileNetV2(weights='imagenet', include_top=False, pooling='avg', input_shape=(img_size, img_size, 3))\n",
        "resnet = ResNet50(weights='imagenet', include_top=False, pooling='avg', input_shape=(img_size, img_size, 3))"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 9,
      "metadata": {
        "id": "GKRk8PKQiL0I"
      },
      "outputs": [],
      "source": [
        "def extract_features(folder, label):\n",
        "    features, labels = [] , []\n",
        "    for fname in os.listdir(folder):\n",
        "        if fname.lower().endswith(('.jpg', '.jpeg', '.png')):\n",
        "            img_path = os.path.join(folder, fname)\n",
        "            img = load_img(img_path, target_size=(img_size, img_size))\n",
        "            arr = img_to_array(img)\n",
        "            arr = np.expand_dims(arr, axis=0)\n",
        "\n",
        "            f1 = mobilenet.predict(mobilenet_preprocess(arr.copy()), verbose=0)\n",
        "            f2 = resnet.predict(resnet_preprocess(arr.copy()), verbose=0)\n",
        "            fused = np.concatenate([f1.flatten(), f2.flatten()])\n",
        "\n",
        "            features.append(fused)\n",
        "            labels.append(label)\n",
        "    return features, labels\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 15,
      "metadata": {},
      "outputs": [],
      "source": [
        "dataset_dir = r\"C:\\Users\\veni\\DFU_dataset\"\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 17,
      "metadata": {},
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "🔍 Checking folder: C:\\Users\\veni\\DFU_dataset\\train\\Healthy foot\n",
            "Exists: True\n",
            "🔍 Checking folder: C:\\Users\\veni\\DFU_dataset\\test\\Healthy foot\n",
            "Exists: True\n",
            "🔍 Checking folder: C:\\Users\\veni\\DFU_dataset\\train\\Ulcered foot\n",
            "Exists: True\n",
            "🔍 Checking folder: C:\\Users\\veni\\DFU_dataset\\test\\Ulcered foot\n",
            "Exists: True\n"
          ]
        }
      ],
      "source": [
        "for cls in [\"Healthy foot\", \"Ulcered foot\"]:\n",
        "    for phase in [\"train\", \"test\"]:\n",
        "        folder = os.path.join(dataset_dir, phase, cls)\n",
        "        print(\"🔍 Checking folder:\", folder)\n",
        "        print(\"Exists:\", os.path.exists(folder))\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 18,
      "metadata": {},
      "outputs": [],
      "source": [
        "X, y = [], []\n",
        "\n",
        "for cls in [\"Healthy foot\", \"Ulcered foot\"]:\n",
        "    for phase in [\"train\", \"test\"]:\n",
        "        folder = os.path.join(dataset_dir, phase, cls)\n",
        "        label = \"healthy\" if \"Healthy\" in cls else \"ulcer\"\n",
        "        ftrs, lbls = extract_features(folder, label)\n",
        "        X.extend(ftrs)\n",
        "        y.extend(lbls)\n",
        "\n",
        "X = np.array(X)\n",
        "y = np.array(y)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 19,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "8a3nz6l3rYE2",
        "outputId": "5c0fa847-38cb-42b5-f52b-2ead777c4131"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "✅ Best Parameters: {'C': 1, 'gamma': 0.001}\n",
            "✅ Best CV Accuracy: 1.0000\n"
          ]
        }
      ],
      "source": [
        "encoder = LabelEncoder()\n",
        "y_encoded = encoder.fit_transform(y)\n",
        "\n",
        "scaler = StandardScaler()\n",
        "X_scaled = scaler.fit_transform(X)\n",
        "\n",
        "svm = SVC(kernel='rbf', probability=True)\n",
        "params = {'C': [1, 10, 100], 'gamma': [0.001, 0.01, 0.1]}\n",
        "\n",
        "grid = GridSearchCV(svm, params, cv=3, scoring='accuracy')\n",
        "grid.fit(X_scaled, y_encoded)\n",
        "\n",
        "print(f\"✅ Best Parameters: {grid.best_params_}\")\n",
        "print(f\"✅ Best CV Accuracy: {grid.best_score_:.4f}\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 21,
      "metadata": {
        "id": "MC2Q5fP09DfQ"
      },
      "outputs": [],
      "source": [
        "from PIL import Image\n",
        "\n",
        "def predict_image(image_path, model, encoder):\n",
        "    img = load_img(image_path, target_size=(224, 224))\n",
        "    arr = img_to_array(img)\n",
        "    arr = np.expand_dims(arr, axis=0)\n",
        "\n",
        "    f1 = mobilenet.predict(mobilenet_preprocess(arr.copy()), verbose=0)\n",
        "    f2 = resnet.predict(resnet_preprocess(arr.copy()), verbose=0)\n",
        "    fused = np.concatenate([f1.flatten(), f2.flatten()]).reshape(1, -1)\n",
        "\n",
        "    fused_scaled = scaler.transform(fused)\n",
        "    pred = model.predict(fused_scaled)[0]\n",
        "    label = encoder.inverse_transform([pred])[0]\n",
        "\n",
        "    print(f\"🖼️ Image: {os.path.basename(image_path)}\")\n",
        "    print(f\"🔍 Prediction: {label.upper()}\")\n",
        "\n",
        "    img = Image.open(image_path)\n",
        "    plt.imshow(img)\n",
        "    plt.axis(\"off\")\n",
        "    plt.title(f\"Prediction: {label}\", color=\"green\" if label == 'healthy' else \"red\")\n",
        "    plt.show()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 516
        },
        "id": "GLMHoyTy9IEm",
        "outputId": "8bfac600-3cf7-4d87-ace9-c3e1211b4dca"
      },
      "outputs": [],
      "source": [
        "# 📂 Replace with the full path to your test image\n",
        "image_path = input(\"Enter the full path to your test image: \")\n",
        "\n",
        "# 👇 Predict using your SVM model\n",
        "predict_image(image_path, grid.best_estimator_, encoder)\n"
      ]
    }
  ],
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "display_name": "base",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.12.7"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}
